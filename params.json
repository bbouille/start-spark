{"name":"Start-spark","tagline":"How to setup Apache Spark","body":"# Getting started with Spark\r\n\r\nThis tutorial was written in June 2014. It covers the version 1.0.0\r\nreleased on may 30, 2014.\r\n\r\n*The tutorial covers Spark setup on a new Ubuntu 14.04 x64 virtual machine :*\r\n\r\n-   Linux prerequisites for Spark\r\n\r\n-   Spark build and installation\r\n\r\n-   Spark configuration\r\n\r\n-   standalone cluster setup (one master and 3 slaves on a single\r\n    machine)\r\n\r\n-   execution of the `PI` approximation (in Scala) job with spark-shell\r\n\r\n\r\n## Dev station setup\r\n\r\n*Before installing Spark:*\r\n\r\n-   Ubuntu 14.04 LTS amd64 (MD5 (ubuntu-14.04-desktop-amd64.iso) =\r\n    dccff28314d9ae4ed262cfc6f35e5153) in a virtual machine (1 cpu / 2 Go\r\n    RAM with Fusion on OSx)\r\n\r\n-   Python 2.7.3 (comming out of box)\r\n\r\nWhat we are going to install :\r\n\r\n-   Oracle JDK 1.7.0\\_60\r\n\r\n-   Scala 2.10.4\r\n\r\n-   Git 1.9.1\r\n\r\n-   Spark 1.0.0\r\n\r\n-   SSH server\r\n\r\n## Introduction\r\n\r\nSpark's slogan is \"lightning-fast cluster computing\" coming from the\r\n[AMPLab at UC Berkeley][].\r\n\r\nThe conception and development started as a research project and turned\r\ninto Apache incubator in june 2013. Graduated an Apache Top-Level\r\nProject in February 2014, this framework has a rapidly growing users and\r\ndevelopers community.\r\n\r\nMainly written in Scala, Spark provides Scala, Java and Python APIs.\r\nScala API provides a way to write concise, higher level routines that\r\neffectively manipulate distributed data.\r\n\r\n## Code teasing\r\n\r\nFor the most impatient, check the examples from the [documentation][] to\r\nsee the simplicity to distribute computation with Scala, Java and Python\r\nAPIs.\r\n\r\n## Corner stone : Resilient Distributed Dataset (RDD)\r\n\r\n*A few words about the Resilient Ditributed Dataset introduced in Spark\r\ncore design.\r\n\r\n*A Resilient Ditributed Dataset is a collection distributed all over the\r\nSpark cluster. RDDs' main purpose is to support higher-level, parallel\r\noperations on data in a straightforward manner. There are currently two\r\ntypes of RDDs: parallelized collections, which take an existing Scala\r\ncollection and run operations on it in parallel, and Hadoop datasets,\r\nwhich run functions on each record of a file in HDFS (or any other\r\nstorage system supported by Hadoop).*\r\n\r\n## Requirements on the environnement (OS side)\r\n\r\nThe following setup follows steps for a 'fresh' Ubuntu installation.\r\nMeaning that it's the first run of my Ubuntu right now.\r\n\r\n### Details of the distribution\r\n\r\n    snoop@ubuntu:~$ uname -a \r\n    # should return:\r\n    Linux ubuntu 3.13.0-23-generic #45-Ubuntu SMP Fri Apr 4 06:58:38 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n    snoop@ubuntu:~$ lsb_release -a\r\n    # should return:\r\n    No LSB modules are available. \r\n    Distributor ID:\tUbuntu \r\n    Description:\tUbuntu Trusty Tahr (development branch) \r\n    Release:\t14.04 \r\n    Codename:\ttrusty\r\n\r\n### Install Java 7\r\n\r\nDue to licence restrictions from Oracle, the sun JDK package is no\r\nlonger available from Ubuntu repositories. Here is a little code snippet\r\nto download and install the Oracle JDK7.\r\n\r\n*(optional) clean up OpenJDK if any version is on your Ubuntu station\r\nnot freshly installed.*\r\n\r\n    snoop@ubuntu:~$ sudo apt-get purge openjdk*\r\n\r\nPrepare the Java7 installation, here is the **\\~/java7.sh** script, open\r\nan editor and add the following :\r\n\r\n    #!/bin/sh -Eux\r\n\r\n    # Add a new repository a your sources lists\r\n    echo \"deb http://ppa.launchpad.net/webupd8team/java/ubuntu precise main\" | tee /etc/apt/sources.list.d/webupd8team-java.list\r\n    echo \"deb-src http://ppa.launchpad.net/webupd8team/java/ubuntu precise main\" | tee -a /etc/apt/sources.list.d/webupd8team-java.list\r\n    apt-key adv --keyserver keyserver.ubuntu.com --recv-keys EEA14886\r\n\r\n    # Update apt-get\r\n    apt-get -y update\r\n    echo oracle-java7-installer shared/accepted-oracle-license-v1-1 select true | sudo /usr/bin/debconf-set-selections\r\n\r\n    # First attempt to install the package\r\n    apt-get -y install oracle-java7-installer\r\n\r\n    # 'lost connection' happen all the time, a quick loop to ensure that network failure do not break the installation process\r\n    while ! apt-get -y install oracle-java7-installer\r\n    do\r\n        sleep 1\r\n    done\r\n\r\nInstall Java7 (taking few minutes) with :\r\n\r\n    snoop@ubuntu:~$ chmod +x java7.sh\r\n    snoop@ubuntu:~$ sudo ./java7.sh\r\n    # should return:\r\n    # [...] many outputs taking few minutes finishing by :\r\n    + apt-get -y install oracle-java7-installer Reading package lists... Done Building dependency tree\r\n    Reading state information... Done\r\n    oracle-java7-installer is already the newest version.\r\n    0 upgraded, 0 newly installed, 0 to remove and 476 not upgraded\r\n\r\nCheck the JAVA path :\r\n\r\n    snoop@ubuntu:~$ file `which java`\r\n    # should return:\r\n    # /usr/bin/java: symbolic link to `/etc/alternatives/java'\r\n\r\n    snoop@ubuntu:~$ file /etc/alternatives/java\r\n    # should return:\r\n    # /etc/alternatives/java: symbolic link to `/usr/lib/jvm/java-7-oracle/jre/bin/java'\r\n\r\nAnd check the java version :\r\n\r\n    snoop@ubuntu:~$ java -version\r\n    # should return:\r\n    # java version \"1.7.0_60\" \r\n    # Java(TM) SE Runtime Environment (build 1.7.0_60-b19) \r\n    # Java HotSpot(TM) 64-Bit Server VM (build 24.60-b09, mixed mode)\r\n\r\n### Set JAVA\\_HOME\r\n\r\nCheck the current \\$JAVA\\_HOME value :\r\n\r\n    snoop@ubuntu:~$ echo $JAVA_HOME\r\n    # should return: nothing\r\n\r\nFrom the Java path checking, set the \\$JAVA\\_HOME for the current user\r\nand load it :\r\n\r\n    snoop@ubuntu:~$ echo \"JAVA_HOME=/usr/lib/jvm/java-7-oracle/\" >> ~/.bashrc\r\n    snoop@ubuntu:~$ source ~/.bashrc\r\n\r\nCheck the new \\$JAVA\\_HOME value :\r\n\r\n    snoop@ubuntu:~$ echo $JAVA_HOME\r\n    # should return:\r\n    # /usr/lib/jvm/java-7-oracle/\r\n\r\nLet’s do it for the whole system with sudo :\r\n\r\n    snoop@ubuntu:~$ sudo bash -c 'echo \"JAVA_HOME=/usr/lib/jvm/java-7-oracle/\" >> /etc/environment'\r\n\r\nHaving problems with Java setup? Check the [ubuntu Java 7\r\ndocumentation][]\r\n\r\n### Install Scala\r\n\r\nSpark 1.0.0 depends on Scala 2.10 and any version 2.10.x should be fine.\r\n\r\nSo we are going to install the latest stable 2.10.4 package from the\r\nofficial page :\r\n\r\n    snoop@ubuntu:~$ wget http://www.scala-lang.org/files/archive/scala-2.10.4.deb\r\n\r\nInstall the `.deb` file :\r\n\r\n    snoop@ubuntu:~$ dpkg -i scala-2.10.4.deb\r\n\r\nCheck the version :\r\n\r\n    snoop@ubuntu:~$ scala -version \r\n    # should return:\r\n    # Scala code runner version 2.10.4 -- Copyright 2002-2013, LAMP/EPFL\r\n\r\n### Set SCALA\\_HOME\r\n\r\nSet the SCALA\\_HOME for the current user, load it and check the variable\r\n:\r\n\r\n    snoop@ubuntu:~$ echo \"SCALA_HOME=/usr/share/java\" >> ~/.bashrc\r\n    snoop@ubuntu:~$ source ~/.bashrc\r\n    snoop@ubuntu:~$ echo $SCALA_HOME\r\n    # should return:\r\n    # /usr/share/java\r\n\r\nDon’t be afraid to set this path, the scala `jar` are there :\r\n\r\n    snoop@ubuntu:~$ ls -al /usr/share/java |grep scala\r\n\r\n## Install Spark\r\n\r\nThere are two options to install Spark :\r\n\r\n-   Install an official build from the [download page][].\r\n\r\nor\r\n\r\n-   Build and install out own from the source.\r\n\r\nSince we need to use the monitoring metrics in Ganglia, we are going to\r\nbuild Spark and explicitly include ganglia (see more [here][])\r\n\r\n### Install git\r\n\r\nWith `apt-get` and `sudo` :\r\n\r\n    snoop@ubuntu:~$ sudo apt-get install git\r\n\r\n### Get the sources\r\n\r\nClone the git repository, go to the spark folder and checkout the\r\nversion 1.0.0:\r\n\r\n    snoop@ubuntu:~$ git clone https://github.com/apache/spark.git\r\n    snoop@ubuntu:~$ cd spark/\r\n    snoop@ubuntu:~/spark$ git checkout v1.0.0\r\n    # should return:\r\n    Note: checking out 'v1.0.0'.\r\n    [...]\r\n    HEAD is now at 2f1dc86... [maven-release-plugin] prepare release v1.0.0-rc11\r\n\r\n### Build Spark\r\n\r\nWe are going to build Spark with [sbt][], a build tool for Scala and\r\nJava, provided in the sources.\r\n\r\nOur Spark build will include two additional features as build options :\r\n\r\n-   Hadoop 2.2.0 support instead of Hadoop 1.0.4 using\r\n    `SPARK_HADOOP_VERSION=2.2.0`\r\n\r\n-   Ganglia sink for monitoring using `SPARK_GANGLIA_LGPL=true`\r\n\r\nTo build Spark with Ganglia and Hadoop 2.2.0, set the environment\r\nvariables in the command line and run the `sbt assembly` command :\r\n\r\n    snoop@ubuntu:~/spark$ SPARK_GANGLIA_LGPL=true SPARK_HADOOP_VERSION=2.2.0 ./sbt/sbt assembly\r\n    # should return:\r\n    # [info] Checking every .class/.jar file's SHA-1. \r\n    [info] SHA-1: da9d2faea382a681d15457e9b01cc96940b6872f \r\n    [info] Packaging /home/snoop/spark/examples/target/scala-2.10/spark-examples-1.0.0-hadoop2.2.0.jar ... \r\n    [info] Done packaging. \r\n    [success] Total time: 2103 s, completed Jun 15, 2014 2:38:25 PM\r\n\r\nOnce it’s done, the easiest way to use Spark is to run the shell (local\r\ninstance) :\r\n\r\n    snoop@ubuntu:~/spark$ ./bin/spark-shell\r\n    # should return:\r\n    14/06/16 00:33:25 INFO SparkILoop: Created spark context.. \r\n    Spark context available as sc.\r\n    scala>\r\n\r\nSome logs are printed in the console for the moment and no error should\r\nhappen. Quit the shell with 'ctrl + d'.\r\n\r\n## Run Spark in standalone mode\r\n\r\nAfter a successful build, it’s time to play (a bit) with Spark in\r\nstandalone mode. In this mode, a master AND several slaves run as\r\nseperate JVM on localhost to simulate a cluster behavior.\r\n\r\n### Set the environnement (Spark side)\r\n\r\nSeveral steps are required before running Spark smoothly in this mode :\r\n\r\n1.  Set up SSH and enable connection without password from localhost to\r\n    localhost for the current user ;\r\n\r\n2.  Set the file `/conf/spark-env.sh` from the template and define the\r\n    number of workers (slaves in JVM).\r\n\r\n3.  Set the `/etc/hosts` with the IP address of eth0 associated to the\r\n    `hostname`\r\n\r\n4.  Set a nicer logger format with log4j to a file\r\n\r\n**Set up and enable SSH connections**\r\n\r\n-   Install and start SSH server with this command :\r\n\r\n<!-- -->\r\n\r\n    snoop@ubuntu:~$ sudo apt-get -y install openssh-server\r\n\r\n-   Generate RSA keys for the current user with `ssh-keygen` and the\r\n    option `-P \"\"` to set a blank password on the private key (BE\r\n    CAREFULL: that disables private key password, do not use in a\r\n    production environment without security measures) :\r\n\r\n<!-- -->\r\n\r\n    snoop@ubuntu:~$ ssh-keygen -P \"\" \r\n    # should return:\r\n    # Generating public/private rsa key pair.\r\n    Enter file in which to save the key (/home/snoop/.ssh/id_rsa): \r\n    Created directory '/home/snoop/.ssh'.\r\n    Your identification has been saved in /home/snoop/.ssh/id_rsa.\r\n    Your public key has been saved in /home/snoop/.ssh/id_rsa.pub.\r\n    The key fingerprint is:\r\n    db:68:50:07:55:13:6a:e6:15:21:74:10:3c:29:d2:d3 snoop@ubuntu\r\n    The key's randomart image is:\r\n    +--[ RSA 2048]----+\r\n    |              o+.|\r\n    |             oo .|\r\n    |            oo.E.|\r\n    |         . o=o+ o|\r\n    |        S o .B.. |\r\n    |         .  o.   |\r\n    |           . ..o |\r\n    |           .o.+  |\r\n    |          ....   |\r\n    +-----------------+\r\n\r\n-   Authorize the key for a connection from localhost to localhost with\r\n    the current user :\r\n\r\n<!-- -->\r\n\r\n    snoop@ubuntu:~$ ssh-copy-id snoop@localhost\r\n    # should return:\r\n    # The authenticity of host 'localhost (127.0.0.1)' can't be established. ECDSA key fingerprint is 75:a7:59:88:34:fc:34:ce:82:f3:4a:3b:d9:0f:4b:e3. \r\n    Are you sure you want to continue connecting (yes/no)? yes /usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed /usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys \r\n    snoop@localhost's password: \r\n    Number of key(s) added: 1\r\n    Now try logging into the machine, with:   \"ssh 'snoop@localhost'\" and check to make sure that only the key(s) you wanted were added.\r\n\r\n-   Check the connection, the following command should not ask for any\r\n    password and return a prompt directly :\r\n\r\n<!-- -->\r\n\r\n    snoop@ubuntu:~$ ssh localhost \r\n    # should return:\r\n    # Welcome to Ubuntu Trusty Tahr (development branch) (GNU/Linux 3.13.0-23-generic x86_64)\r\n    Documentation:  https://help.ubuntu.com/\r\n\r\nThen exit from the SSH connection :\r\n\r\n    snoop@ubuntu:~$ exit \r\n    logout Connection to localhost closed.\r\n\r\n\\\r\n**Set the Spark environnement**\r\n\r\nCopy the template from conf/spark-env.sh.template to conf/spark-env.sh :\r\n\r\n    snoop@ubuntu:~$ cd spark\r\n    snoop@ubuntu:~/spark$ cp conf/spark-env.sh.template conf/spark-env.sh\r\n\r\nAnd set the number of workers at 3 with the enviroment variable\r\n`SPARK_WORKER_INSTANCES` :\r\n\r\n    snoop@ubuntu:~/spark$ echo \"export SPARK_WORKER_INSTANCES=3\" >> ./conf/spark-env.sh\r\n\r\n**Set the /etc/hosts**\r\n\r\nThe workers and the master communicate through the network and require a\r\nvalid DNS entries. In standalone mode, a DNS server is not required and\r\nthese entries can be set manually in the /etc/hosts file.\r\n\r\nRun `hostname` command to be sure :\r\n\r\n    snoop@ubuntu:~/spark$ hostname\r\n    # should return:\r\n    # ubuntu\r\n\r\nCheck your IP address on eth0 (assigned by DHCP in default\r\nconfiguration) :\r\n\r\n    snoop@ubuntu:~/spark$ ifconfig | perl -nle'/dr:(\\S+)/ && print $1' | head -n 1\r\n    # should return:\r\n    # 192.168.67.190\r\n\r\nThen edit your `/etc/hosts` to look like the following (you can remove\r\n127.0.1.1 and IPv6 sections) :\r\n\r\n    snoop@ubuntu:~/spark$ cat /etc/hosts \r\n    127.0.0.1   localhost \r\n    192.168.67.190  ubuntu\r\n\r\n*NOTE*: the important point here is that the hostname (ubuntu) **must**\r\nbe bound to the external IP (192.168.67.190)\r\n\r\n**Set the log4j logger**\r\n\r\nWe need to redirect the logs to a file instead of the console (displayed\r\nin the shell), be careful to set your path for the key\r\n`log4j.appender.FILE.File` :\r\n\r\n    snoop@ubuntu:~/spark$ cp conf/log4j.properties.template conf/log4j.properties\r\n    snoop@ubuntu:~/spark$ vim conf/log4j.properties\r\n    # Initialize root logger\r\n    log4j.rootLogger=INFO, FILE\r\n    log4j.rootCategory=INFO, FILE\r\n\r\n    # Set the appender named FILE to be a File appender\r\n    log4j.appender.FILE=org.apache.log4j.FileAppender\r\n\r\n    # Change the path to where you want the log file to reside\r\n    log4j.appender.FILE.File=/home/snoop/spark/logs/SparkOut.log\r\n\r\n    # Prettify output a bit\r\n    log4j.appender.FILE.layout=org.apache.log4j.PatternLayout\r\n    log4j.appender.FILE.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\r\n\r\n    Settings to quiet third party logs that are too verbose log4j.logger.org.eclipse.jetty=WARN log4j.logger.org.apache.spark.repl.SparkIMainexprTyper=INFO log4j.logger.org.apache.spark.repl.SparkILoopSparkILoopInterpreter=INFO\r\n\r\n### Start the master\r\n\r\nUse the `start-master.sh` script :\r\n\r\n    snoop@ubuntu:~/spark$ ./sbin/start-master.sh  \r\n    # should return:\r\n    # starting org.apache.spark.deploy.master.Master, logging to /home/snoop/spark/sbin/../logs/spark-snoop-org.apache.spark.deploy.master.Master-1-ubuntu.out\r\n\r\nCheck the process with `jps` :\r\n\r\n    $ jps \r\n    # should return:\r\n    # 40334 Master \r\n      40322 Jps\r\n\r\nCheck the master web UI on your browser : [http://localhost:8080][]\r\n![][1]\r\n\r\n### Start the 3 slaves\r\n\r\nUse the `start-slave.sh` script :\r\n\r\n    snoop@ubuntu:~/spark$ ./sbin/start-slaves.sh -h spark://ubuntu:7077\r\n    # should return:\r\n    # localhost: starting org.apache.spark.deploy.worker.Worker, logging to /home/snoop/spark/sbin/../logs/spark-snoop-org.apache.spark.deploy.worker.Worker-1-ubuntu.out localhost: starting org.apache.spark.deploy.worker.Worker, logging to /home/snoop/spark/sbin/../logs/spark-snoop-org.apache.spark.deploy.worker.Worker-2-ubuntu.out localhost: starting org.apache.spark.deploy.worker.Worker, logging to /home/snoop/spark/sbin/../logs/spark-snoop-org.apache.spark.deploy.worker.Worker-3-ubuntu.out\r\n\r\nCheck the process with `jps` :\r\n\r\n    snoop@ubuntu:~/spark$ jps\r\n    # should return:\r\n    # 41376 Jps \r\n    40334 Master \r\n    41297 Worker \r\n    41058 Worker \r\n    40831 Worker\r\n\r\n![][2]\r\n\r\n### Start/stop the master and the workers in one shoot\r\n\r\nStop the master and the workers:\r\n\r\n    snoop@ubuntu:~/spark$ ./sbin/stop-all.sh\r\n\r\n\\\r\nStart again :\r\n\r\n    snoop@ubuntu:~/spark$ ./sbin/start-all.sh\r\n\r\n## Playing with Spark (shell)\r\n\r\nFinally we reach the intersting part ! Welcome to Spark and enjoy the\r\njourney :)\r\n\r\n### Fire up the shell\r\n\r\nTime to run the console against the cluster :\r\n\r\n    snoop@ubuntu:~/spark$ MASTER=spark://ubuntu:7077 ./bin/spark-shell\r\n\r\nEt voilà !\r\n\r\nSpark up and ready at home for interactive commande (read *Scala\r\ncommandes*) :\r\n\r\n![][3]\r\n\r\n### Hello Spark\r\n\r\nA quick implementation of Pi approximation suggested by [mbonaci][] is a\r\ngood example. Write the following code in the spark console :\r\n\r\n    /* throwing darts and examining coordinates */\r\n    val NUM_SAMPLES = 100000\r\n    val count = sc.parallelize(1 to NUM_SAMPLES).map{i =>\r\n      val x = Math.random * 2 - 1\r\n      val y = Math.random * 2 - 1\r\n      if (x * x + y * y < 1) 1.0 else 0.0\r\n    }.reduce(_ + _)\r\n\r\n    println(\"Pi is roughly \" + 4 * count / NUM_SAMPLES)\r\n\r\nAnd hit ‘enter’ to see the result :\r\n\r\n![][4]\r\n\r\nYou can now use the Spark Context and test some code in standalone mode.\r\n\r\nThe next article will focus on application developpement giving details\r\non code writing with Eclipse, build with sbt, submission to this\r\nstandalone cluster.\r\n\r\nThanks for reading.\r\n\r\n# Reference\r\n\r\nThe tutorial was (greatly) inspired by [mbonaci][]'s exploration on\r\nversion 0.9.0. Thanks for sharing.\r\n\r\n  [AMPLab at UC Berkeley]: https://amplab.cs.berkeley.edu\r\n  [2]: https://spark.apache.org/docs/0.9.1/quick-start.html#a-standalone-app-in-scala\r\n  [documentation]: http://spark.apache.org/examples.html\r\n  [ubuntu Java 7 documentation]: https://help.ubuntu.com/community/Java#Oracle_Java_7\r\n  [download page]: http://spark.apache.org/downloads.html\r\n  [here]: http://spark.apache.org/docs/latest/monitoring.html#metrics\r\n  [sbt]: http://www.scala-sbt.org/\r\n  [http://localhost:8080]: #\r\n  [mbonaci]: https://github.com/mbonaci/mbo-spark\r\n  [1]: https://raw.githubusercontent.com/bbouille/start-spark/master/src/spark-master-start.png\r\n  [2]: https://raw.githubusercontent.com/bbouille/start-spark/master/src/spark-start.png\r\n  [3]: https://raw.githubusercontent.com/bbouille/start-spark/master/src/spark-shell-standalone.png\r\n  [4]: https://raw.githubusercontent.com/bbouille/start-spark/master/src/spark-pi.png\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}